# ğŸš€ API da LIA - Backend

API REST da LIA (Luminnus Intelligent Assistant) para processamento de mensagens de chat usando OpenAI GPT-3.5-turbo.

## ğŸ“‹ Endpoints

### `GET /`
Verifica se a API estÃ¡ ativa.

**Resposta:**
```
LIA Chat API ativa!
```

### `GET /health`
Endpoint de health check para monitoramento.

**Resposta:**
```json
{
  "status": "ok",
  "message": "API estÃ¡ online"
}
```

### `POST /chat`
Envia uma mensagem para a LIA e recebe a resposta processada.

**Request Body:**
```json
{
  "message": "OlÃ¡, LIA!"
}
```

**Resposta de Sucesso (200):**
```json
{
  "reply": "OlÃ¡! Como posso ajudar vocÃª hoje?"
}
```

**Resposta de Erro (400):**
```json
{
  "error": "Mensagem nÃ£o fornecida."
}
```

**Resposta de Erro (500):**
```json
{
  "error": "Erro interno no servidor."
}
```

## ğŸ”§ ConfiguraÃ§Ã£o Local

### 1. Instalar dependÃªncias
```bash
cd api
npm install
```

### 2. Configurar variÃ¡veis de ambiente
Crie um arquivo `.env` na pasta `api`:

```env
OPENAI_API_KEY=sua_chave_openai_aqui
PORT=3000
```

### 3. Executar servidor
```bash
npm start
```

O servidor estarÃ¡ disponÃ­vel em `http://localhost:3000`

## ğŸŒ Deploy no Render

### Passo 1: Preparar o repositÃ³rio
Certifique-se de que os arquivos `api/server.js` e `api/package.json` estÃ£o commitados no repositÃ³rio.

### Passo 2: Criar novo Web Service no Render
1. Acesse [https://render.com](https://render.com)
2. Clique em **"New +"** â†’ **"Web Service"**
3. Conecte seu repositÃ³rio GitHub
4. Configure o serviÃ§o:
   - **Name:** `lia-chat-api`
   - **Region:** `Oregon (US West)` ou mais prÃ³ximo
   - **Branch:** `main` (ou sua branch principal)
   - **Root Directory:** `api`
   - **Runtime:** `Node`
   - **Build Command:** `npm install`
   - **Start Command:** `npm start`
   - **Instance Type:** `Free` (ou superior)

### Passo 3: Configurar variÃ¡veis de ambiente
No painel do Render, vÃ¡ em **Environment** e adicione:

- **OPENAI_API_KEY**: sua chave da OpenAI (obrigatÃ³rio)
  - Obtenha em: https://platform.openai.com/api-keys
  - Formato: `sk-proj-...`

### Passo 4: Deploy
Clique em **"Create Web Service"** e aguarde o deploy finalizar.

Sua API estarÃ¡ disponÃ­vel em:
```
https://lia-chat-api.onrender.com
```

### Passo 5: Testar
Acesse a URL do seu serviÃ§o no navegador. VocÃª deve ver:
```
LIA Chat API ativa!
```

Teste o health check:
```bash
curl https://lia-chat-api.onrender.com/health
```

Teste o chat:
```bash
curl -X POST https://lia-chat-api.onrender.com/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "OlÃ¡, LIA!"}'
```

## âš™ï¸ ConfiguraÃ§Ãµes do Render

### Auto-Deploy
O Render faz deploy automÃ¡tico quando vocÃª faz push para a branch configurada.

### Sleep Mode (Free Tier)
No plano gratuito, o serviÃ§o "dorme" apÃ³s 15 minutos de inatividade:
- A primeira requisiÃ§Ã£o pode demorar ~30 segundos
- Para evitar, considere fazer upgrade para plano pago

### Logs
Acesse os logs em tempo real no painel do Render:
1. Clique no seu serviÃ§o
2. VÃ¡ em **"Logs"**

### Monitoramento
- **Health Check URL:** `/health`
- Configure alertas no painel do Render

## ğŸ”’ SeguranÃ§a

### VariÃ¡veis de Ambiente
- âœ… **NUNCA** commite a `OPENAI_API_KEY` no cÃ³digo
- âœ… Use variÃ¡veis de ambiente no Render
- âœ… Mantenha as chaves seguras e rotacione periodicamente

### CORS
A API permite requisiÃ§Ãµes de qualquer origem (`cors()` sem restriÃ§Ãµes).
Para produÃ§Ã£o, considere restringir:

```javascript
app.use(cors({
  origin: 'https://seu-frontend.com'
}));
```

### Rate Limiting
Para produÃ§Ã£o, adicione rate limiting:

```bash
npm install express-rate-limit
```

```javascript
import rateLimit from 'express-rate-limit';

const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutos
  max: 100 // mÃ¡ximo de 100 requisiÃ§Ãµes por IP
});

app.use('/chat', limiter);
```

## ğŸ“Š Custos OpenAI

O modelo `gpt-3.5-turbo` tem os seguintes custos (aproximados):
- **Input:** $0.50 / 1M tokens
- **Output:** $1.50 / 1M tokens

Monitore seu uso em: https://platform.openai.com/usage

## ğŸ› ï¸ Troubleshooting

### Erro 401 "Incorrect API key"
- Verifique se a `OPENAI_API_KEY` estÃ¡ configurada corretamente no Render
- Confirme que a chave estÃ¡ ativa em https://platform.openai.com/api-keys

### Erro "Cannot find module"
- Certifique-se de que o `package.json` estÃ¡ correto
- Verifique se o Build Command estÃ¡ como `npm install`

### Timeout / 504 Gateway Timeout
- A OpenAI pode demorar para responder mensagens longas
- Considere aumentar o timeout ou adicionar streaming

### API nÃ£o responde
- Verifique os logs no painel do Render
- Confirme que o serviÃ§o estÃ¡ "Running" (nÃ£o "Sleeping" ou "Failed")
- Teste o endpoint `/health` primeiro

## ğŸ“š DocumentaÃ§Ã£o OpenAI

- [API Reference](https://platform.openai.com/docs/api-reference)
- [Chat Completions](https://platform.openai.com/docs/guides/chat)
- [Rate Limits](https://platform.openai.com/docs/guides/rate-limits)

## ğŸ”„ AtualizaÃ§Ãµes Futuras

PossÃ­veis melhorias:
- [ ] Adicionar streaming de respostas
- [ ] Implementar cache de respostas
- [ ] Adicionar suporte a imagens (GPT-4 Vision)
- [ ] Implementar histÃ³rico de conversas
- [ ] Adicionar analytics e mÃ©tricas
- [ ] Suporte a mÃºltiplos modelos (GPT-4, Claude, etc.)

## ğŸ“ LicenÃ§a

Este projeto faz parte do sistema proprietÃ¡rio Luminnus.
Todos os direitos reservados Â© 2025 Luminnus.
